
--------------------------------------------------------
template event generator - standard output (metadata shown)
--------------------------------------------------------

input {
    exec {
        command => "echo 'hello'"
        interval => 10
    }
}

filter {

}

output {
    stdout {
        codec => rubydebug { metadata => true }
    }
}

----------------------------
template elastic - elastic
----------------------------

input {
    elasticsearch {
        hosts => [ "elastic:9200" ]
        index => "source"
        docinfo => true
        docinfo_fields => ["_index", "_id"]
        docinfo_target => "_originatingWLIndex"
    }
}

filter {
    mutate {
        add_field => {
            "[@metadata][_id]" => "%{[_originatingWLIndex][_id]}"
            "[@metadata][_type]" => "_doc"
            "[@metadata][_index]" => "%{[_originatingWLIndex][_index]}"
        }
    }
}

output {
    elasticsearch {
        hosts => [ "elastic:9200" ]
        index => "target"
        document_type => "_doc"
        document_id => "%{[@metadata][_id]}"
    }
}

----------------------------
CSV file input
----------------------------

input {
    file {
        path => "/path_to_source/source.csv"
        type => "csv"
    }
}

----------------------------
XML file input
----------------------------

input {
    file {
        path => "/path_to_source/source.xml"
        type => "xml"
    }
}

----------------------------
Kafka input (JSON)
----------------------------

    kafka {
        bootstrap_servers => "kafka-broker:9092"
        topics => ["my.consumed.topic"]
    }

----------------------------
Kafka input (AVRO from registry)
----------------------------

// needs plugin
// https://github.com/salsify/logstash-codec-avro_schema_registry
RUN bin/logstash-plugin install logstash-codec-avro_schema_registry

input {
    kafka {
        bootstrap_servers => "kafka-broker:9092"
        codec => avro_schema_registry {
            endpoint => "http://kafka-schema-registry:9010"
            tag_on_failure => true
        }
        consumer_threads => 1
        topics => ["my.consumed.topic"]
        value_deserializer_class => "org.apache.kafka.common.serialization.ByteArrayDeserializer"
        decorate_events => true
    }
}

// decorate_events => true              // appends Kafka specific metadata to logstash events

----------------------------
elastic input with query
----------------------------

input {
    elasticsearch {
        hosts => [ "elastic:9200" ]
        query => '{"query":{"query_string":{"query":"name:Tomas"}}}'
        index => "source"
        docinfo => true
        docinfo_fields => ["_index", "_id"]
        docinfo_target => "_originatingWLIndex"
    }
}

----------------------------
Elastic output with pipeline
----------------------------

output {
    elasticsearch {
        hosts => [ "elastic:9200" ]
        index => "target"
        pipeline => "some_pipeline"
        document_type => "_doc"
        document_id => "%{[@metadata][_id]}"
    }
}

----------------------------
Kafka output (JSON)
----------------------------

output {
    kafka {
        bootstrap_servers => "kafka-broker:9092"
        codec=> "plain"
        topic_id => "my.produced.topic"
    }
}

----------------------------
Kafka output (AVRO from registry)
----------------------------

// needs plugin
// https://github.com/salsify/logstash-codec-avro_schema_registry
RUN bin/logstash-plugin install logstash-codec-avro_schema_registry


output {
	kafka {
        bootstrap_servers => "kafka-broker:9092"
		topic_id => "my.produced.topic"
		value_serializer => "org.apache.kafka.common.serialization.ByteArraySerializer"
		message_key => "%{[@metadata][_id]}"
        codec => avro_schema_registry {
			endpoint => "http://kafka-schema-registry:9010"
			subject_name => "my.schema.subject"
            schema_version => 1
            register_schema => true
			# seems to be serialized already
			binary_encoded => false
		}
	}
}

----------------------------
transformations / filters
----------------------------

	# simple concatenate
	mutate {
        add_field => { "target_field" => "%{source_field_1} %{source_field_2}"}
    }

    # multiple fields can be added
    mutate {
        add_field => {
            "target_field1" => "%{source_field_1}"
            "target_field2" => "%{source_field_2}"
        }
    }
	
	mutate {
		copy => { "source_field" => "source_field_copy" }
	}
	
	# works on object
	# does not work on object arrays or nested object arrays
	mutate {
        add_field => { "target_field" => "%{[source_object_field][source_object_sub_field]}"}
    }

	# works on nested object, having at least 1 item
	# works on object array, having at least 1 item
	# does not work on object
	mutate {
        add_field => { "target_field" => "%{[source_nested_object_field][0][source_object_sub_field]}"}
	}

	mutate {
		rename => {"[source_field]" => "[target_field]"}
    }

	mutate {
		remove_field => [ "source_field" ]
	}

	# works on object
	# does not work on object arrays or nested object arrays
	mutate {
		remove_field => [ "[source_object_field][source_object_sub_field]" ]
	}

	# equivalent code for sub-field removal
    ruby {
        code => '
            original_nested_object_array = event.get("NESTED");
            new_nested_object_array = Array.new()
            original_nested_object_array.each { |original_nested_object|
                original_nested_object.delete("an_objects_field");
                new_nested_object_array.push(original_nested_object);
            }
            event.set("NESTED", new_nested_object_array)
        '
    }

	# create field (if it does not exist) with empty string value
	if !([field]) {
		mutate {
			add_field => { "field" => ""}
		}
	}
	
--------------------------
other
--------------------------

logstash -f my_configuration.conf --path.data /usr/share/logstash/my_data_folder --debug

logstash-plugin list
logstash-plugin install logstash-filter-prune
logstash-plugin install logstash-codec-avro_schema_registry
logstash-plugin install logstash-codec-avro


